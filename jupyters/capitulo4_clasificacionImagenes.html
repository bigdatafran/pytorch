

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>8. Introducción clasificación de imágenes. &#8212; Trabajando con PyTorch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=927b94d3fcb96560df09" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=927b94d3fcb96560df09" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=927b94d3fcb96560df09" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=927b94d3fcb96560df09" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=927b94d3fcb96560df09" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=927b94d3fcb96560df09" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=927b94d3fcb96560df09"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'jupyters/capitulo4_clasificacionImagenes';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Introducción. Modelos preentrenados." href="ModelosPreentrenados.html" />
    <link rel="prev" title="7. Introducción problemas de clasificación." href="Tema_3_clasificacion.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../introduccion.html">
  
  
  
  
  
    <p class="title logo__title">Trabajando con PyTorch</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../introduccion.html">
                    Introducción
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Pycharm</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="OperacionesTensores.html">1. Los tensores en PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="Calcularderivadas.html">2. Introducción a PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="OptimizadoresPyTorch.html">3. Optimizadores en PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tema1regresionlineal.html">4. Regresión lineal.</a></li>

<li class="toctree-l1"><a class="reference internal" href="Tema%202.html">6. Regresión lineal. Continuación</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tema_3_clasificacion.html">7. Problemas de clasificación</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. Introducción clasificación de imágenes.</a></li>


<li class="toctree-l1"><a class="reference internal" href="ModelosPreentrenados.html">11. TorchVision-Modelos preentrenados</a></li>
<li class="toctree-l1"><a class="reference internal" href="Capitulo4bisEspacioFeatures.html">12. Espacio Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="Capitulo5Convoluciones.html">13. Introducción a las convoluciones.</a></li>












<li class="toctree-l1"><a class="reference internal" href="Tema11_NLP.html">26. Lenguaje natural</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Apéndice</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Apendice.html">27. Apéndice</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Índice de términos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">28. Índice de términos</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/jupyters/capitulo4_clasificacionImagenes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introducción clasificación de imágenes.</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">8. Introducción clasificación de imágenes.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formacion-de-imagenes-para-tensores">8.1. Formación de imágenes para tensores.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formato-nchw-vs-nhwc">8.2. Formato NCHW vs NHWC.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchvision">8.3. Torchvision.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets">8.3.1. Datasets.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos">8.3.2. Modelos.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformadores">8.3.3. Transformadores.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformaciones-sobre-imagenes">8.3.4. Transformaciones sobre imágenes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformaciones-sobre-tensores">8.3.5. Transformaciones sobre tensores.</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformacion-normalize">8.3.5.1. Transformación normalize.</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#composicion-de-transformaciones">8.3.6. Composición de transformaciones.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparacion-de-los-datos">8.4. Preparación de los datos.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subsetrandomsampler">8.5. SubsetRandomSampler.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformacion-para-aumentar-los-datos">8.6. Transformación para aumentar los datos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weightedrandomsampler">8.7. WeightedRandomSampler.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#las-semillas-seeds">8.8. Las semillas (seeds)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pixeles-y-features">8.9. Pixeles y features.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-shallow-modelo-poco-profundo">8.10. Modelo Shallow (modelo poco profundo).</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notacion-utilizada">8.11. Notación utilizada.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-mas-profundo">8.11.1. Modelo más profundo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#el-modelo-matematico-subyacente">8.11.2. El modelo matemático subyacente.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pesos-como-pixeles">8.11.3. Pesos como pixeles.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#funciones-de-activacion">9. Funciones de activación.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#funcion-sigmoidea">9.1. Función sigmoidea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tangente-hiperbolica-tanh">9.2. Tangente hiperbólica (TanH).</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rectified-linear-unit-relu">9.3. Rectified Linear Unit (ReLU)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu">9.4. Leaky ReLU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-relu-prelu">9.5. Parametric ReLU (PReLU)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-de-aprendizaje">10. Modelo de aprendizaje.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formulacion-matematica-del-problema">10.1. Formulación matemática del problema.</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduccion-clasificacion-de-imagenes">
<h1><span class="section-number">8. </span>Introducción clasificación de imágenes.<a class="headerlink" href="#introduccion-clasificacion-de-imagenes" title="Permalink to this heading">#</a></h1>
<p>Si en el tema anterior hemos procedido a crear código para poder clasificar elementos numéricos, en el tema actual vamos a proceder a hacer una clasificación similar, pero en este caso con imágenes.</p>
<p>Lo primero que hacemos, es importar los paquetes que necesitamos</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#!pip install torchvision</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">random_split</span><span class="p">,</span> \
        <span class="n">WeightedRandomSampler</span><span class="p">,</span> <span class="n">SubsetRandomSampler</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="n">Normalize</span><span class="p">,</span>\
<span class="n">ToPILImage</span><span class="p">,</span> <span class="n">RandomHorizontalFlip</span><span class="p">,</span> <span class="n">Resize</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">stepbystep.v0</span> <span class="kn">import</span> <span class="n">StepByStep</span>
</pre></div>
</div>
</div>
</div>
<p>Vamos a definir en el bloque siguiente una función que nos servirá para generar las imágenes con las que se va a trabajar</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gen_img</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">img_size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Generates empty image</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">img_size</span><span class="p">,</span> <span class="n">img_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="n">start_row</span><span class="p">,</span> <span class="n">start_col</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">start</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">start_row</span> <span class="o">=</span> <span class="n">start</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">start_col</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">start</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">start_row</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">img</span><span class="p">[:,</span> <span class="n">start_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">fill</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">img</span><span class="p">[</span><span class="n">start_row</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">fill</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">start_col</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">start_col</span> <span class="o">=</span> <span class="mi">1</span>
        
        <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">start_row</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">up</span> <span class="o">=</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">start_row</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> 
                      <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">start_row</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">up</span> <span class="o">=</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">img_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">start_col</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> 
                      <span class="nb">range</span><span class="p">(</span><span class="n">start_col</span><span class="p">,</span> <span class="n">img_size</span><span class="p">))</span>
            <span class="n">img</span><span class="p">[</span><span class="n">up</span><span class="p">]</span> <span class="o">=</span> <span class="n">fill</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">start_row</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">down</span> <span class="o">=</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">start_row</span><span class="p">,</span> <span class="n">img_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
                        <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">img_size</span> <span class="o">-</span> <span class="n">start_row</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">down</span> <span class="o">=</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">img_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">start_col</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> 
                        <span class="nb">range</span><span class="p">(</span><span class="n">start_col</span><span class="p">,</span> <span class="n">img_size</span><span class="p">))</span>
            <span class="n">img</span><span class="p">[</span><span class="n">down</span><span class="p">]</span> <span class="o">=</span> <span class="n">fill</span>
    
    <span class="k">return</span> <span class="mi">255</span> <span class="o">*</span> <span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">img_size</span><span class="p">,</span> <span class="n">img_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">generate_dataset</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_images</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">17</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">starts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">img_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">img_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_images</span><span class="p">,))</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_images</span><span class="p">,))</span>
    
    <span class="n">images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gen_img</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">)</span> 
                       <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">starts</span><span class="p">,</span> <span class="n">targets</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="p">(</span><span class="n">targets</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">images</span><span class="p">,</span> <span class="n">targets</span>
</pre></div>
</div>
</div>
</div>
<p>Las imágenes que se generan con el código anterior, son muy simples, pero nos sirven para mostrar el procedimiento de clasificación de este tipo de elementos. Consisten en imágenes que tienen el fondo negro y después unas líneas blancas que están dibujadas sobre ese fondo negro. La clasificación que se va a hacer es la siguiente:</p>
<ul class="simple">
<li><p>Si la línea es diagonal, entonces toma valor 1 (clase positiva)</p></li>
<li><p>Si la línea no es diagonal, entonces toma el valor 0 (clase negativa)</p></li>
</ul>
<p>Vamos a generar de forma aleatoria un total de 300 imágenes cada una con un total de 5x5 pixeles</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_dataset</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_images</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">n_plot</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">n_rows</span> <span class="o">=</span> <span class="n">n_plot</span> <span class="o">//</span> <span class="mi">6</span> <span class="o">+</span> <span class="p">((</span><span class="n">n_plot</span> <span class="o">%</span> <span class="mi">6</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">n_rows</span><span class="p">))</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">images</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">],</span> <span class="n">targets</span><span class="p">[:</span><span class="n">n_plot</span><span class="p">])):</span>
        <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">i</span> <span class="o">//</span> <span class="mi">6</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">6</span>    
        <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;#</span><span class="si">{}</span><span class="s1"> - Label:</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">target</span><span class="p">),</span> <span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">12</span><span class="p">})</span>
        <span class="c1"># plot filter channel in grayscale</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">label_outer</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">n_plot</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/dc0f9d38015226c707208fb7d270ae4b47dcc2aca2284c8cde6631a9d08dcff2.png" src="../_images/dc0f9d38015226c707208fb7d270ae4b47dcc2aca2284c8cde6631a9d08dcff2.png" />
</div>
</div>
<p>Dado el escaso número de pixeles con los que trabajamos, existirán imágenes repetidas en el total de 300 que hemos generado,pero esto en principio no importa.</p>
<section id="formacion-de-imagenes-para-tensores">
<h2><span class="section-number">8.1. </span>Formación de imágenes para tensores.<a class="headerlink" href="#formacion-de-imagenes-para-tensores" title="Permalink to this heading">#</a></h2>
<p>En este apartado vamos a hacer un pequeño inciso sobre el formato que tienen las imágenes para poder trabajar sobre ellas con los tensores.</p>
<p>Para entrar en escena, vamos a crear a continuación tres imágenes de 5x5 pixeles, que está representado por una matriz de tamaño 5x5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">image_r</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span>
<span class="n">image_r</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">image_g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">image_g</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">image_g</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span>
<span class="n">image_g</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">image_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">image_b</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">image_b</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_r</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[255, 128,   0,   0,   0],
       [255, 128,   0,   0,   0],
       [255, 128,   0,   0,   0],
       [255, 128,   0,   0,   0],
       [255, 128,   0,   0,   0]], dtype=uint8)
</pre></div>
</div>
</div>
</div>
<p>Tenemos en este caso tres representaciones de una figura en dos dimensiones lo que significa que se tiene una representación de la imagen en un canal simple. Los valores de la matriz varían desde 0 ( color negro) hasta el 255 (color blanco) y entre medias tendríamos las diferentes tonalidades de grises.</p>
<p>Veamos algunas representaciones de estas imágenes</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_r</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_g</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_b</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x22d8b5eada0&gt;
</pre></div>
</div>
<img alt="../_images/62135f4d4db2f9a148ac2ec5521c84ffcf019242ff238a2303c95fe82a1946d6.png" src="../_images/62135f4d4db2f9a148ac2ec5521c84ffcf019242ff238a2303c95fe82a1946d6.png" />
</div>
</div>
<p>Cada una de las matrices anteriores podrían representar un color: rojo, verde y amarillo que al mezclarse darían lugar a los diferentes colores que podemos apreciar los humanos. La mezcla de estos colores, lo hariamos de la siguiente manera:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_rgb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">image_r</span><span class="p">,</span> <span class="n">image_g</span><span class="p">,</span> <span class="n">image_b</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">image_rgb</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[[255,   0,   0],
        [128, 128,   0],
        [  0, 255,   0],
        [  0, 128, 128],
        [  0,   0, 255]],

       [[255,   0,   0],
        [128, 128,   0],
        [  0, 255,   0],
        [  0, 128, 128],
        [  0,   0, 255]],

       [[255,   0,   0],
        [128, 128,   0],
        [  0, 255,   0],
        [  0, 128, 128],
        [  0,   0, 255]],

       [[255,   0,   0],
        [128, 128,   0],
        [  0, 255,   0],
        [  0, 128, 128],
        [  0,   0, 255]],

       [[255,   0,   0],
        [128, 128,   0],
        [  0, 255,   0],
        [  0, 128, 128],
        [  0,   0, 255]]], dtype=uint8)
</pre></div>
</div>
</div>
</div>
</section>
<section id="formato-nchw-vs-nhwc">
<h2><span class="section-number">8.2. </span>Formato NCHW vs NHWC.<a class="headerlink" href="#formato-nchw-vs-nhwc" title="Permalink to this heading">#</a></h2>
<p id="index-0">El significado de esta numenclatura es el siguiente:</p>
<ul class="simple">
<li><p>N: Representa el número de imágenes.</p></li>
<li><p>C Representa el número de canales.</p></li>
<li><p>H Representa la altura de la imagen (Height)</p></li>
<li><p>W Representa la anchura de la imagen (Widht)</p></li>
</ul>
<p>Esta notación es el acrónimo de lo que se espera en una entrada mini-batch.</p>
<p>Entonces:</p>
<ul class="simple">
<li><p>NCHW : (n.imagenes,canales,alto,ancho)</p></li>
<li><p>NHWC : (n.imagenes,alto,ancho,canales)</p></li>
</ul>
<p>Hay que tener en cuenta que el formato NCHW es el utilizado por PyTorch, NHWC lo utiliza NHWC  el <a href="https://pillow.readthedocs.io/en/stable/index.html" target="_blank"> paquete de imágenes PIL </a> usa el formato HWC.</p>
<p>Teniendo en cuenta todos estos conceptos, lógicamente las imágenes que hemos generado están en formato NCHW listas por lo tanto para poder ser usadas en PyTorch. Veamos pues la dimensión del conjunto de imágenes que hemos creado.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">images</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(300, 1, 5, 5)
</pre></div>
</div>
</div>
</div>
<p>Es decir, tendriamos 300 imágenes de un solo canal, de 5x5. Veamos el contenido de una imagen</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span>
<span class="n">example</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[[  0, 255,   0,   0,   0],
        [  0,   0, 255,   0,   0],
        [  0,   0,   0, 255,   0],
        [  0,   0,   0,   0, 255],
        [  0,   0,   0,   0,   0]]], dtype=uint8)
</pre></div>
</div>
</div>
</div>
<p>O varias imágenes</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">images</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="mi">7</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[[[  0,   0,   0,   0,   0],
         [  0,   0,   0,   0, 255],
         [  0,   0,   0, 255,   0],
         [  0,   0, 255,   0,   0],
         [  0, 255,   0,   0,   0]]],


       [[[  0, 255,   0,   0,   0],
         [  0,   0, 255,   0,   0],
         [  0,   0,   0, 255,   0],
         [  0,   0,   0,   0, 255],
         [  0,   0,   0,   0,   0]]]], dtype=uint8)
</pre></div>
</div>
</div>
</div>
<p>En todas estas imágenes, vemos que todas ellas tienen una línea diagonal, indicada por el valor 255 (color blanco). Supongamos que ahora queremos pasar una imagen al formato requerido por PIL (HWC), entonces para conseguir estos debemos usar el método <em>transpose</em> de numpy de las siguiente manera:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_hwc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">example</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="n">example_hwc</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5, 5, 1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_hwc</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[[  0],
        [255],
        [  0],
        [  0],
        [  0]],

       [[  0],
        [  0],
        [255],
        [  0],
        [  0]],

       [[  0],
        [  0],
        [  0],
        [255],
        [  0]],

       [[  0],
        [  0],
        [  0],
        [  0],
        [255]],

       [[  0],
        [  0],
        [  0],
        [  0],
        [  0]]], dtype=uint8)
</pre></div>
</div>
</div>
</div>
</section>
<section id="torchvision">
<h2><span class="section-number">8.3. </span>Torchvision.<a class="headerlink" href="#torchvision" title="Permalink to this heading">#</a></h2>
<p><a href="https://pillow.readthedocs.io/en/stable/index.html" target="_blank"> Esta librería </a> es parte del proyecto PyTorch y contiene data set populares, arquitectura de modelos, y transformaciones comunes de imágenes para computer vision.</p>
<section id="datasets">
<h3><span class="section-number">8.3.1. </span>Datasets.<a class="headerlink" href="#datasets" title="Permalink to this heading">#</a></h3>
<p id="index-1">Muchos data set populares, como MNIST, ImageNet o CIFAR están incluidos en los dataset de PyTorch. Estos data set heredan de la clase <em>Dataset</em>, y en consecuencia se les puede aplicar <em>DataLoader</em> como ya se ha explicado en apartados anteriores.</p>
<p>Hay un particular dataset sobre el que hay que pararse un poquito. Se trata de <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder"  target="_blank"> ImageFolder </a>, que es un dataset genérico que se puede utilizar con las propias imágenes del ususario. Este dataset se puede organizar en subcarpetas, cada una conteniendo imágenes de diversos temas.</p>
</section>
<section id="modelos">
<h3><span class="section-number">8.3.2. </span>Modelos.<a class="headerlink" href="#modelos" title="Permalink to this heading">#</a></h3>
<p>PyTorch también incluye <a herf="https://pytorch.org/vision/stable/models.html" target="_blank"> diversos modelos ya entrenados </a>.</p>
<p>Entre los muchos modelos que ya vienen en la instalación por defecto, se pueden encontrar algunos bien conocidos como Alexnet, VGG, ResNet.</p>
</section>
<section id="transformadores">
<h3><span class="section-number">8.3.3. </span>Transformadores.<a class="headerlink" href="#transformadores" title="Permalink to this heading">#</a></h3>
<p>Se puede encontrar también un módulo que permite <a heref="https://pytorch.org/vision/stable/transforms.html" target="_blank"> hacer transformaciones de imágenes </a>. Este módulo es importante para hacer dos grupos de transformaciones:</p>
<ul class="simple">
<li><p>Transformaciones sobre imágenes. (Hay un trasnformador para convertir entre imágenes PIL y tensores)</p></li>
<li><p>Trasnformaciones basadas en tensores.</p></li>
</ul>
<p id="index-2">Se pueden hacer transformaciones de tensor a PIL (ToPILImage), y de PIL a tensor (ToTensor)</p>
<p>Vamos a ver cómo poder convertir una imagen PIL, que es la que hemos obtenido antes y que se denomina <em>example_hwc</em> a un tensor de PyTorch. Lo hacemos de la siguiente manera</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tensorizer</span> <span class="o">=</span> <span class="n">ToTensor</span><span class="p">()</span> <span class="c1"># Esta clase pasa a tensor la imagen con formato PIL</span>
<span class="n">example_tensor</span> <span class="o">=</span> <span class="n">tensorizer</span><span class="p">(</span><span class="n">example_hwc</span><span class="p">)</span>
<span class="n">example_tensor</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 5, 5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Veamos su contenido</span>
<span class="n">example_tensor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0., 1., 0., 0., 0.],
         [0., 0., 1., 0., 0.],
         [0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 1.],
         [0., 0., 0., 0., 0.]]])
</pre></div>
</div>
</div>
</div>
<p>Podemos observar que aquí también tenemos una imagen en diagonal, pero…no es lo mismo que teniamos en la imagen inicial, pues recordemos que aquella tenía de contenido lo siguiente</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[[  0, 255,   0,   0,   0],
        [  0,   0, 255,   0,   0],
        [  0,   0,   0, 255,   0],
        [  0,   0,   0,   0, 255],
        [  0,   0,   0,   0,   0]]], dtype=uint8)
</pre></div>
</div>
</div>
</div>
<p>Es decir el valor 255 se ha convertido en 1. Esto es así porque por defecto esta transformación lo que ha hecho es escalar los valores, para que pasen a valer entre cero y uno.</p>
<p>Estas transformaciones modifican las imágenes de entrenamiento de muchas formas diferentes: girando, desplazando, volteando, recortando, desenfocando ampliarla, añadirle ruido, borrar partes de ella…</p>
<p><strong>Es lo que se llama aumento de datos</strong>. Es una técnica inteligente para ampliar un conjunto de datos (aumentarlo) sin recoger más datos. En general, los modelos de aprendizaje profundo son muy ávidos de datos y requieren una gran cantidad de ejemplos para funcionar bien. Pero la recopilación de grandes conjuntos de datos suele ser difícil, y a veces imposible.</p>
<p>Para entender mejor lo anterior supongamos que tenemos una imagen de un perro. Si la giramos, sigue siendo un perro,
pero desde un ángulo diferente. En lugar de tomar dos fotos del perro, una desde cada ángulo, tomamos la imagen que ya tenemos y utilizamos el aumento de datos para simular muchos ángulos diferentes. No es lo mismo que la foto real, pero se acerca lo suficiente como para mejorar el rendimiento de nuestro modelo.</p>
<p>Ni que decir tiene que el aumento de datos no es adecuado para todas las tareas: si se trata de realizar una detección de objetos es decir, detectar la posición de un objeto en una imagen, no debería hacer nada que cambie su posición, como voltear o desplazar. Sin embargo, añadir un poco de ruido también estaría bien.</p>
</section>
<section id="transformaciones-sobre-imagenes">
<h3><span class="section-number">8.3.4. </span>Transformaciones sobre imágenes<a class="headerlink" href="#transformaciones-sobre-imagenes" title="Permalink to this heading">#</a></h3>
<p>Para ver este apartado es necesario inicialmente tener una imagen y para ello veamos ahora cómo poder convertir un tensor a una imagen PIL. El código sería el siguiente</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example_img</span> <span class="o">=</span> <span class="n">ToPILImage</span><span class="p">()(</span><span class="n">example_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">example_img</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;PIL.Image.Image&#39;&gt;
</pre></div>
</div>
</div>
</div>
<p>Como el valor obtenido es una imagen PIL podremos usar Matpltliib para hacer su representación gráfica.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">example_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d9315cf20bb124a5699af9ad008afe48b6229a162ad079c5a09f5135b71212b7.png" src="../_images/d9315cf20bb124a5699af9ad008afe48b6229a162ad079c5a09f5135b71212b7.png" />
</div>
</div>
<p>Son muchas las transformaciones que se pueden hacer sobre una imagen y que se pueden consultar en la documentación oficial de PyTorch, pero aquí vamos a ver un ejemplo sobre cómo poder utilizar estas herramientas.</p>
<p>Entre las transformaciones que nos ofrece pytorch están: Resize(), CenterCrop, GrayScale, RandomHorizontalFlip y RandomRotation.</p>
<p>Vamos a utilizar la transformación <em>RandomHorizontalFlip</em>. En este ejemplo no vamos a dejar nada aleatorio y lo que vamos a hacer es que la figura se gire por completo. Veamoslo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">flipper</span> <span class="o">=</span> <span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">flipped_img</span> <span class="o">=</span> <span class="n">flipper</span><span class="p">(</span><span class="n">example_img</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">flipped_img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f2ff071d0585928e4c03a838b320fcea7a3999e636bdd750f4c72fe78dac6e84.png" src="../_images/f2ff071d0585928e4c03a838b320fcea7a3999e636bdd750f4c72fe78dac6e84.png" />
</div>
</div>
<p>Como podemos ver es como si tuviéramos un eje vertical en el centro de la imagen y se hiciera girar esa imagen 180 grados alrededor de ese eje.</p>
</section>
<section id="transformaciones-sobre-tensores">
<h3><span class="section-number">8.3.5. </span>Transformaciones sobre tensores.<a class="headerlink" href="#transformaciones-sobre-tensores" title="Permalink to this heading">#</a></h3>
<p>Para el caso de transformaciones para tensores (a diferencia de las imágenes), tan sólo tenemos cuatro posibilidades:</p>
<p>1.- LinearTransformation</p>
<p>2.- Normalize</p>
<p>3.- RandomErasing</p>
<p>4.- ConvertImageDtype</p>
<p>Para ver el funcionamiento de estas transformaciones, vamos a pasar la imagen a un tensor</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">img_tensor</span> <span class="o">=</span> <span class="n">tensorizer</span><span class="p">(</span><span class="n">flipped_img</span><span class="p">)</span>
<span class="n">img_tensor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0., 0., 0., 1., 0.],
         [0., 0., 1., 0., 0.],
         [0., 1., 0., 0., 0.],
         [1., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
</pre></div>
</div>
</div>
</div>
<section id="transformacion-normalize">
<h4><span class="section-number">8.3.5.1. </span>Transformación normalize.<a class="headerlink" href="#transformacion-normalize" title="Permalink to this heading">#</a></h4>
<p>La funcionalidad de esta transformación se <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize" target="_blank"> puede encontrar en este enlace </a>. De acuerdo con la documentación oficial, lo que se hace es normalizar un tensor de imagen mediante una media y una desviación típica, dadas cada una para un determinado canal.</p>
<p>Observemos que en nuestro caso los valores de los pixeles son positivos y menor que 255, o menor que 1 cuando hemos obtenido los valores de los tensores de la imagen. Supongamos que ahora a estos último valores (los comprendidos entre cero y uno) les hacemos esta transformación <em>normalize</em> para una media igual a 0.5 y desviación estándar de 0.5. En esta situación el valor cero que es el más pequeño se convierte en -1 y el 1 en 1. Es decir es como si hacemos la transformación <em>MinMaxScaler</em> de Scikit learn.</p>
<p>En cambio, si tomáramos como desviación estándar 0.25, el rango de valores estaría entre [-2,2]. Obtendriamos una distribución asimétrica si tomáramos como valor medio un valor diferente al punto medio del rango de valores.</p>
<p>Vamos ahora a hacer mediante código una normalización y veamos que obtenemos valores comprendidos entre -1 y 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,))</span>
<span class="n">normalized_tensor</span> <span class="o">=</span> <span class="n">normalizer</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>
<span class="n">normalized_tensor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-1., -1., -1.,  1., -1.],
         [-1., -1.,  1., -1., -1.],
         [-1.,  1., -1., -1., -1.],
         [ 1., -1., -1., -1., -1.],
         [-1., -1., -1., -1., -1.]]])
</pre></div>
</div>
</div>
</div>
<p>Recordemos el valor de img_tensor</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">img_tensor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[0., 0., 0., 1., 0.],
         [0., 0., 1., 0., 0.],
         [0., 1., 0., 0., 0.],
         [1., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
</pre></div>
</div>
</div>
</div>
<p>Efectivamente, donde antes había un cero ahora vale -1 y un 1 se queda tal y como estaba.</p>
<p>Observar que la transformación toma dos tuplas como argumentos, una para las medias y otra para las desviaciones estándar. Cada tupla tupla tiene tantos valores como canales en la imagen. Como en este ejemplo tenemos imágenes de un solo canal, nuestras tuplas tienen un solo elemento cada una.</p>
</section>
</section>
<section id="composicion-de-transformaciones">
<h3><span class="section-number">8.3.6. </span>Composición de transformaciones.<a class="headerlink" href="#composicion-de-transformaciones" title="Permalink to this heading">#</a></h3>
<p>Normalmente en los trabajos de deeplearning con imágenes lo que se hace son varias transformaciones, entonces para incluir todas ellas en una sola instrucción, lo que se hace es utilizar la clase <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" target="_blank"> <em>Compose</em> </a>.</p>
<p>Anteriormente hemos puesto en marcha dos transformaciones de forma aislada: <em>RandomHorizontalFlip</em> y <em>Normalize</em>. Hagamos las dos transformaciones de una vez creando una composición.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">composer</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
<span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,))])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">composed_tensor</span> <span class="o">=</span> <span class="n">composer</span><span class="p">(</span><span class="n">example_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="preparacion-de-los-datos">
<h2><span class="section-number">8.4. </span>Preparación de los datos.<a class="headerlink" href="#preparacion-de-los-datos" title="Permalink to this heading">#</a></h2>
<p>Procedemos en este apartado a seguir trabajando con los datos sobre imágenes que hemos creado al comienzo del mismo. Como los valores que toman las matrices están comprendidos entre 0 y 255, lo que hacemos es transformarlos para que tomen valores entre cero y uno</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">images</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="c1">#¡¡¡¡¡NOTA: Observar que tenemos que hacer ese reshape para tener el formato necesario de los datos</span>
<span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Vamos a crear nuestro propio dataset en el cual se va a implementar transformaciones sobre los datos.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformedTensorDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        El valor de x serían las features, y serían los labels .</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Por defectos no hay transformación alguna</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>
        
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="c1"># si se indica alguna transformación la hacemos</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># devuelve el x trasnformado y el label y que le corresponde</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Vamos a definir una composición de transformaciones, del mismo modo que lo hemos hecho en un ejemplo anteriormente, y esa transformación se la pasamos como parámetro al dataset construido anteriormente.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">composer</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
<span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,))])</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TransformedTensorDataset</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">,</span> <span class="n">y_tensor</span><span class="p">,</span> <span class="n">composer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="subsetrandomsampler">
<h2><span class="section-number">8.5. </span>SubsetRandomSampler.<a class="headerlink" href="#subsetrandomsampler" title="Permalink to this heading">#</a></h2>
<p id="index-3">Anteriormente, al crear un cargador de datos para el conjunto de entrenamiento, utilizábamos el argumento “shuffle” a “True” (ya que barajar los puntos de datos, en la mayoría de los casos, mejora el rendimiento del descenso de gradiente).</p>
<p>Esta era una forma muy conveniente de barajar los datos, que se implementa usando un RandomSampler bajo el capó. Cada vez que se necesitaba un  un nuevo minilote, se muestreaban algunos índices al azar, y se devolvían los puntos de datos correspondientes a esos índices.</p>
<p>Incluso cuando no había barajado de los datos, como en el cargador de datos utilizado para el conjunto de validación, se utilizó un SequentialSampler. En este, cada vez que se requería un nuevo minilote, este muestreador simplemente devolvía una secuencia de índices, en orden, y los puntos de datos correspondientes a esos índices.</p>
<p>En pocas palabras, un muestreador puede utilizarse para devolver secuencias de índices que se utilizarán para la carga de datos. En los dos ejemplos anteriores cada muestreador tomaría un conjunto de datos como argumento. Pero no todos los
muestreadores son así.</p>
<p>El <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler" target="_blank"> SubsetRandomSampler </a> muestrea índices de una lista, dada como como argumento, sin reemplazo. Como en los otros muestreadores, estos índices se utilizarán para cargar datos de un conjunto de datos. Si un índice no está
en la lista, nunca se utilizará el punto de datos correspondiente.</p>
<p>Así, si tenemos dos listas de índices disjuntos (es decir, que no hay intersección entre ellas, y cubren todos los elementos si se procede a su unión), podemos crear dos muestreadores para dividir efectivamente un conjunto de datos. Pongamos esto en código, para que quede más claro.</p>
<p>En primer lugar, tenemos que generar dos listas de índices barajadas, una correspondiente a los puntos del conjunto de entrenamiento, y la otra, a los puntos del conjunto de validación. Esto ya lo hemos hecho utilizando
Numpy. Hagámoslo un poco más interesante y útil esta vez montando la función auxiliar que sigue, que llamaremos <em>index_splitter</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">index_splitter</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">splits</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Función auxiliar que sirve para crear listas de elementos de train y de test</span>
<span class="sd">    </span>
<span class="sd">    Los parámetros de esta función son::</span>
<span class="sd">    * n: el número total de datos</span>
<span class="sd">    * splits: Una lista de valores indicando el peso de datos de entrenamiento y de validación</span>
<span class="sd">    * seed: La semilla a tener en cuenta</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="c1"># Makes the split argument a tensor</span>
    <span class="n">splits_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">splits</span><span class="p">)</span>
    <span class="c1"># Finds the correct multiplier, so we don&#39;t have</span>
    <span class="c1"># to worry about summing up to N (or one)</span>
    <span class="n">multiplier</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">splits_tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>    
    <span class="n">splits_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">multiplier</span> <span class="o">*</span> <span class="n">splits_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
    <span class="c1"># If there is a difference, throws at the first split</span>
    <span class="c1"># so random_split does not complain</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">splits_tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">splits_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">diff</span>
    <span class="c1"># Uses PyTorch random_split to split the indices</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">random_split</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">splits_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hacemos las dos listas, una con el 80% de puntos y la otra con el20%</span>
<span class="n">train_idx</span><span class="p">,</span> <span class="n">val_idx</span> <span class="o">=</span> <span class="n">index_splitter</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">),</span> <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
<span class="n">train_idx</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch.utils.data.dataset.Subset at 0x22d8936a3e0&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sacamos unos pocos valores para ver su contenido</span>
<span class="n">train_idx</span><span class="o">.</span><span class="n">indices</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[118, 170, 148, 239, 226, 146, 168, 195, 6, 180]
</pre></div>
</div>
</div>
</div>
<p>Ahora ya sí estamos en disposición de utilizar la función de PyTorch <em>SubsetRandomSampler</em> y así poder extraer los índices que conformarán los datos de entrenamiento</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_sampler</span> <span class="o">=</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="n">train_idx</span><span class="p">)</span>
<span class="n">val_sampler</span> <span class="o">=</span> <span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="n">val_idx</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora ya solo nos quedaría utilizar <em>DataLoader</em> para cargar los datos de prueba y validación</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Builds a loader of each set</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">val_sampler</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="transformacion-para-aumentar-los-datos">
<h2><span class="section-number">8.6. </span>Transformación para aumentar los datos<a class="headerlink" href="#transformacion-para-aumentar-los-datos" title="Permalink to this heading">#</a></h2>
<p>Otra de las razones por la que podemos seguir necesitando dos conjuntos de datos divididos es exactamente esa: el aumento de datos.</p>
<p>En general, queremos aplicar el aumento de datos sólo a los datos de entrenamiento (sí, también hay aumento de datos de prueba, pero eso es otro asunto).</p>
<p>Este aumento de los datos se puede realizar mediante una transformación compuesta que la vamos a aplicar a todos los puntos del conjunto de datos.</p>
<p>Si necesitamos que se aumenten algunos puntos de datos, pero no otros, la manera más fácil de conseguirlo es crear dos compositores y utilizarlos en dos conjuntos de datos diferentes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train_tensor</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">y_train_tensor</span> <span class="o">=</span> <span class="n">y_tensor</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>

<span class="n">x_val_tensor</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span>
<span class="n">y_val_tensor</span> <span class="o">=</span> <span class="n">y_tensor</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Creamos entonces dos composer. Uno que se aplica al conjunto de prueba y que aumenta los datos y al mismo tiempo los escala (min-max), y otro que sólo escala los datos.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_composer</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">.5</span><span class="p">),</span>
                          <span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,))])</span>

<span class="n">val_composer</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,))])</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora ya estamos en disposición de utilizar los <em>composer</em> anteriores para  generar los dos conjuntos de datos de la siguiente manera</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TransformedTensorDataset</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">,</span> <span class="n">y_train_tensor</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">train_composer</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">TransformedTensorDataset</span><span class="p">(</span><span class="n">x_val_tensor</span><span class="p">,</span> <span class="n">y_val_tensor</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">val_composer</span><span class="p">)</span>

<span class="c1"># Builds a loader of each set</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="weightedrandomsampler">
<h2><span class="section-number">8.7. </span>WeightedRandomSampler.<a class="headerlink" href="#weightedrandomsampler" title="Permalink to this heading">#</a></h2>
<p id="index-4">Ya hemos tocado el tema de los conjuntos de datos desequilibrados al hablar de las clasificaciones binarias un capítulo anterior. Ajustamos el peso de la pérdida para los puntos de la clase positiva para compensar el desequilibrio existente. No era la media ponderada que cabría. Ahora, podemos abordar el problema del desequilibrio de los datos utilizando un enfoque diferente: un muestreador ponderado.</p>
<p>El razonamiento es más o menos el mismo pero, en lugar de pérdidas ponderadas, utilizamos pesos para el muestreo, que significa lo siguiente.</p>
<p>La clase con menos puntos de datos(clase minoritaria) debería tener una mayor ponderación, mientras que la clase
con más puntos de datos (clase mayoritaria) debería tener pesos más pequeños.</p>
<p>De este modo, en promedio, acabaremos con minilotes(mini-batch) que contienen aproximadamente el mismo número de puntos de datos en cada clase: un conjunto de datos equilibrado.</p>
<p>Para hacer lo anterior y en primer lugar, tenemos que averiguar el grado de desequilibrio del conjunto de datos, es decir, cuántos puntos de datos pertenecen a cada etiqueta o clase. Para ello, podemos utilizar el método <em>unique</em> de PyTorch en las etiquetas de nuestro conjunto de entrenamiento (y_train_tensor), con el parámetro <em>return_counts</em> igual a True, para obtener una lista de las etiquetas existentes y el correspondiente número de puntos de datos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classes</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">y_train_tensor</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 1.]) tensor([ 80, 160])
</pre></div>
</div>
</div>
</div>
<p>Por lo tanto tendriamos 80 imágenes con líneas que no son diagonales y 160 con líneas diagonales. Por lo tanto quedaría claro que sí estamos ante un problema de clasificación con datos no balanceados.</p>
<p>Vamos ahora a calcular los pesos que deberiamos asignar a cada clase de forma inversamente proporcional al número de elementos que hay en cada una de ellas</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">weights</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0125, 0.0063])
</pre></div>
</div>
</div>
</div>
<p>Es habitual tener pesos que sumen uno, seguro, pero esto no es requerido por el muestreador ponderado de PyTorch. Podemos salirnos con la suya con pesos inversamente proporcionales a los recuentos.</p>
<p>No basta con proporcionar una secuencia de pesos correspondientes a cada clase diferente del conjunto de entrenamiento. Se requiere una secuencia que contenga el peso correspondiente a todos y cada uno de los datos del conjunto de entrenamiento.</p>
<p>Aunque esto es un poco molesto, no es difícil de lograr: podemos utilizar las etiquetas como índices de los pesos pesos que hemos calculado anteriormente. Probablemente sea más fácil verlo en el código:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">y_train_tensor</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sample_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_weights</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train_tensor</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([240])
tensor([0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0125, 0.0063, 0.0063, 0.0063,
        0.0063])
tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])
</pre></div>
</div>
</div>
</div>
<p>Esta secuencia de pesos es el argumento principal de la <em>WeightedRandomSampler</em>, pero además tiene otros que a continuación pasamos a enunciar y aclarar su significado.</p>
<ul class="simple">
<li><p><em>weights</em>: Una secuencia de pesos, similar a lo que acabamos de generar</p></li>
<li><p><em>num_samples</em>: cuántas muestras se van a extraer del conjunto de datos.Un valor típico puede ser la longitud de la secuencia de pesos, ya que es muy probable que estés muestreando de todo el conjunto de entrenamiento.</p></li>
<li><p><em>replacement</em>: Si es igual a True (valor por defecto), genera listas con reemplazaminto</p>
<ul>
<li><p>Si <em>num_samples</em> es igual a la longitud, es decir si se utiliza el conjunto de datos total, tiene sentido extraer muestras con reemplazo para compensar eficazmente el desequilibrio</p></li>
<li><p>sólo tiene sentido ponerlo en False si num_samples &lt; longitud del conjunto de datos</p></li>
</ul>
</li>
<li><p><em>generator</em>: Es opcional, toma un (pseudo) generador de números aleatorios que se utilizará para extraer las muestras.</p></li>
<li><p><em>to</em> : Para garantizar la reproducibilidad, tenemos que crear y asignar un generador (que tiene su propia semilla) al muestreador, ya que la semilla manual que ya hemos establecido no es suficiente.</p></li>
</ul>
<p>De acuerdo con esto, vamos a muestrear de todo el conjunto de entrenamiento, y ya tenemos nuestra secuencia de pesos lista. Sin embargo, todavía nos falta un generador. Vamos a crear tanto el generador como el sampler ahora.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>

<span class="n">sampler</span> <span class="o">=</span> <span class="n">WeightedRandomSampler</span><span class="p">(</span>
    <span class="n">weights</span><span class="o">=</span><span class="n">sample_weights</span><span class="p">,</span>
    <span class="n">num_samples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_weights</span><span class="p">),</span>
    <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
    <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora ya podemos generar el conjunto de entrenamiento y de prueba</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Con el fin de agrupar todo lo dicho anteriormente, construimos una función que engloba el código que anteriormente se ha implementado.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_balanced_sampler</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="c1"># Computes weights for compensating imbalanced classes</span>
    <span class="n">classes</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">sample_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
    <span class="c1"># Builds sampler with compute weights</span>
    <span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">WeightedRandomSampler</span><span class="p">(</span>
        <span class="n">weights</span><span class="o">=</span><span class="n">sample_weights</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_weights</span><span class="p">),</span>
        <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
        <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">sampler</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sampler</span> <span class="o">=</span> <span class="n">make_balanced_sampler</span><span class="p">(</span><span class="n">y_train_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="las-semillas-seeds">
<h2><span class="section-number">8.8. </span>Las semillas (seeds)<a class="headerlink" href="#las-semillas-seeds" title="Permalink to this heading">#</a></h2>
<p>ES el momento de establecer la semilla para el generador utilizado en el muestreador asignado al cargador de datos. Es una larga secuencia de objetos, pero podemos podemos recorrerla para recuperar el generador y llamar a su método
método manual_seed:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora podemos comprobar si nuestro muestreador está haciendo su trabajo correctamente. Vamos a muestrear una carrera completa (240 puntos de datos en 15 mini-lotes de 16 puntos cada uno), y sumemos las etiquetas para saber cuántos puntos
están en la clase positiva:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(123.)
</pre></div>
</div>
</div>
</div>
<p>Casi… Tenemos 160 imágenes de la clase positiva, y ahora, gracias al muestreador ponderado, estamos muestreando sólo 123 de ellas. Esto significa que estamos sobremuestreando la clase negativa (que tiene 80 imágenes) a un total de 117 imágenes, sumando 240 imágenes. Misión lograda, nuestro conjunto de datos está ahora equilibrado.</p>
<p>Estoy de acuerdo, demasiadas semillas. Además de una semilla específica para el generador, también tenemos que establecer otra semilla para el módulo <a href="https://docs.python.org/3/library/random.html" target="_blank"> random de Python </a>.</p>
<p>Más vale prevenir que curar, así que será mejor que pongamos otra semilla para asegurar la reproducibilidad de nuestro código.</p>
<p>A continuación ampliamos la función <em>set_seed</em> de la clase <em>StepByStep</em> que hemos creado en un capítulo anterior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>    
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">pass</span>
   
<span class="nb">setattr</span><span class="p">(</span><span class="n">StepByStep</span><span class="p">,</span> <span class="s1">&#39;set_seed&#39;</span><span class="p">,</span> <span class="n">set_seed</span><span class="p">)</span>

<span class="c1"># setattr cambia algún metodo de una determinada clase</span>
</pre></div>
</div>
</div>
</div>
<p>¡Cuatro semillas y contando! Nuestro método actualizado trata de actualizar la semilla del generador utilizado por el muestreador asignado al cargador de datos del conjunto de entrenamiento. Pero, si no hay generador (el argumento
es opcional, después de todo), falla silenciosamente.</p>
<p>Hemos comentado y dicho por muchas cosas en cuanto a la preparación de los datos. Pongámoslas todas juntas para tener una mejor visión del panorama general.</p>
<p>En primer lugar, hemos construido un conjunto de datos personalizado para manejar las transformaciones en
tensores, y dos funciones de ayuda  para dividir los índices y construir un muestreo aleatorio ponderado.</p>
<p>A continuación, realizamos diferentes pasos de procesamiento para preparación:</p>
<ul class="simple">
<li><p>Modificación de la escala de [0,255] a [0,1]</p></li>
<li><p>División de índices y tensores en dos conjuntos: validación y test</p></li>
<li><p>Construcción de una transformación compuesta incluyendo aumento de los datos en el conjunto de entrenamiento.</p></li>
<li><p>Uso de un conjunto de datos personalizado para aplicar transformaciones a los tensores.</p></li>
<li><p>creación de un muestreador aleatorio ponderado para tratar el desequilibrio de clases</p></li>
<li><p>Creación de data loaders, utilizando el muestreador junto con el conjunto de entrenamiento</p></li>
</ul>
<p>El código final que hemos obtenido es el siguiente:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Builds tensors from numpy arrays BEFORE split</span>
<span class="c1"># Modifies the scale of pixel values from [0, 255] to [0, 1]</span>
<span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">images</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="c1"># Uses index_splitter to generate indices for training and</span>
<span class="c1"># validation sets</span>
<span class="n">train_idx</span><span class="p">,</span> <span class="n">val_idx</span> <span class="o">=</span> <span class="n">index_splitter</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">),</span> <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>

<span class="c1"># Uses indices to perform the split</span>
<span class="n">x_train_tensor</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">y_train_tensor</span> <span class="o">=</span> <span class="n">y_tensor</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">x_val_tensor</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span>
<span class="n">y_val_tensor</span> <span class="o">=</span> <span class="n">y_tensor</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span>

<span class="c1"># Builds different composers because of data augmentation on training set</span>
<span class="n">train_composer</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">.5</span><span class="p">),</span>
                          <span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,))])</span>
<span class="n">val_composer</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,))])</span>
<span class="c1"># Uses custom dataset to apply composed transforms to each set</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TransformedTensorDataset</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">,</span> <span class="n">y_train_tensor</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">train_composer</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">TransformedTensorDataset</span><span class="p">(</span><span class="n">x_val_tensor</span><span class="p">,</span> <span class="n">y_val_tensor</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">val_composer</span><span class="p">)</span>

<span class="c1"># Builds a weighted random sampler to handle imbalanced classes</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">make_balanced_sampler</span><span class="p">(</span><span class="n">y_train_tensor</span><span class="p">)</span>

<span class="c1"># Uses sampler in the training set to get a balanced data loader</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Con esto hemos terminado el apartado de preparación de los datos, aunque queda por comentar lo siguiente.</p>
</section>
<section id="pixeles-y-features">
<h2><span class="section-number">8.9. </span>Pixeles y features.<a class="headerlink" href="#pixeles-y-features" title="Permalink to this heading">#</a></h2>
<p>Hasta ahora, hemos manejado nuestros datos como imágenes PIL o tensores tridimensionales (CHW) con dimensiones (1, 5, 5). También es posible considerar cada píxel y canal como una feature individual aplanando los píxeles con una capa de aplanamiento (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html" target="_blank"> Flatten layer </a>).</p>
<p>Vamos a tomar un mini-batch de imágenes de nuestro conjunto de entrenamiento para ilustrar cómo es este método.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_xs</span><span class="p">,</span> <span class="n">dummy_ys</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">dummy_xs</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([16, 1, 5, 5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">flattener</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="n">dummy_xs_flat</span> <span class="o">=</span> <span class="n">flattener</span><span class="p">(</span><span class="n">dummy_xs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dummy_xs_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dummy_xs_flat</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([16, 25])
tensor([-1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,
        -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.])
</pre></div>
</div>
</div>
</div>
<p>Por defecto, preserva la primera dimensión de forma que mantenemos el número de puntos de datos en el mini-batch, pero colapsa las dimensiones restantes. Si observamos el primer elemento del mini-batch aplanados, encontramos un tensor largo con 25 (1 x 5 x 5) elementos en él. Si nuestras imágenes tuvieran tres canales, el tensor sería sería de 75 (3 x 5 x 5) elementos.</p>
<p>Ahora bien haciendo esto no perderemos información? Pues claro que sí, y por eso las redes neuronales convolucionales
(CNN), que veremos en el próximo capítulo, tienen tanto éxito: no pierden esta información. Pero, por ahora, vamos a hacerlo a la antigua usanza y a aplanar los píxeles. Esto hará que sea mucho más sencillo ilustrar un par de conceptos más antes de llegar a las CNN más sofisticadas.</p>
<p>Antes,en los capítulos anteriores, nuestros puntos de datos eran tensores con uno o dos elementos, es decir, una o dos características. Ahora,al aplanar el conjunto de datos que son la base de las imágenes ,nuestros puntos de datos son
tensores con 25 elementos, cada uno de los cuales corresponde a un píxel/canal de la imagen original, como si fueran 25 “features”.</p>
<p>Entonces como los datos no son diferentes a los visto en capítulos anteriores, aparte del número de características o features, podemos  partir de lo que ya sabemos sobre la definición de un modelo para para manejar una tarea de clasificación binaria.</p>
</section>
<section id="modelo-shallow-modelo-poco-profundo">
<h2><span class="section-number">8.10. </span>Modelo Shallow (modelo poco profundo).<a class="headerlink" href="#modelo-shallow-modelo-poco-profundo" title="Permalink to this heading">#</a></h2>
<p>Este modelo no es otra cosa que un modelo de regresión logística:</p>
<div class="math notranslate nohighlight">
\[
\Large \text{P}(y=1) = \sigma(z) = \sigma(w_0x_0+w_1x_1+\cdots+w_{24}x_{24})
\]</div>
<p>Dadas 25 features, de x0 a x24, cada una de las cuales corresponde al valor de un píxel en un canal determinado, el modelo ajustará una regresión lineal de forma que sus resultados sean logits (z), que se convierten en probabilidades mediante una función sigmoidea.</p>
<p>Esta sección la hemos llamado modelo superficial por una razón… en la siguiente, construiremos un modelo más profundo con una capa oculta en él. Esto qué significa?. Lo podemos ver en la siguiente imagen</p>
<p><img alt="Modelo dos capas" src="../_images/ModeloDosCapas.PNG" /></p>
<p>En este modelo se ha quitado el parámetro <em>bias</em>. Quiero ilustrar la diferencia entre un modelo superficial, y uno más profundo y es mucho más fácil de resolver si nos deshacemos del sesgo.</p>
<p>Además, también me gustaría recordar la notación correspondiente a nuestro modelo, ya que estoy planeando utilizar esta notación para ilustrar este punto.</p>
</section>
<section id="notacion-utilizada">
<h2><span class="section-number">8.11. </span>Notación utilizada.<a class="headerlink" href="#notacion-utilizada" title="Permalink to this heading">#</a></h2>
<p>Veamos en forma matricial, cómo definimos este problema</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Large W =
\underset{(25 \times 1)}{
\begin{bmatrix}
w_0 \\
w_1 \\
\vdots \\
w_{24}
\end{bmatrix}};
X = 
\underset{(25 \times 1)}{
\begin{bmatrix}
x_0 \\
x_1 \\
\vdots \\
x_{24}
\end{bmatrix}}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\Large 
\begin{aligned}
z
&amp; = W^T \cdot X 
=\underset{(1 \times 25)}{
\begin{bmatrix}
- &amp; w^{T} &amp; -\\
\end{bmatrix}}
\underset{(25 \times 1)}{
\begin{bmatrix}
x_0 \\
x_1 \\
\vdots \\
x_{24}
\end{bmatrix}} 
=
\underset{(1 \times 25)}{
\begin{bmatrix}
w_0 &amp; w_1 &amp; \cdots &amp; w_{24}
\end{bmatrix}}
\underset{(25 \times 1)}{
\begin{bmatrix}
x_0 \\
x_1 \\
\vdots \\
x_{24}
\end{bmatrix}}
\\
&amp; = w_0x_0 + w_1x_1 + \cdots + w_{24}x_{24}
\end{aligned}
\end{split}\]</div>
<p>Como es habitual, sólo tenemos que definir un modelo, una función de pérdida adecuada y un optimizador. Como ahora tenemos imágenes de cinco por cinco pixeles y un sólo canal como entradas, tenemos que aplanarlas primero, para que puedan ser entradas adecuadas para nuestra capa lineal (sin sesgo). Como optimizador seguiremos utilizando el optimizador SGD con una tasa de aprendizaje de 0,1 por por ahora.</p>
<p>Todo esto lo hacemos con el código siguiente</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sets learning rate - this is &quot;eta&quot; ~ the &quot;n&quot; like Greek letter</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="c1"># Now we can create a model</span>
<span class="n">model_logistic</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model_logistic</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;flatten&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model_logistic</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_logistic</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>

<span class="c1"># Defines a SGD optimizer to update the parameters </span>
<span class="n">optimizer_logistic</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_logistic</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># Defines a binary cross entropy loss function</span>
<span class="n">binary_loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Vamos a entrenar ahora nuestro modelo durante 100 epochs utilizando la clase StepByStep y visualizan la evolución de la función de pérdida.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">sbs_logistic</span> <span class="o">=</span> <span class="n">StepByStep</span><span class="p">(</span><span class="n">model_logistic</span><span class="p">,</span> <span class="n">binary_loss_fn</span><span class="p">,</span> <span class="n">optimizer_logistic</span><span class="p">)</span>
<span class="n">sbs_logistic</span><span class="o">.</span><span class="n">set_loaders</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
<span class="n">sbs_logistic</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sbs_logistic</span><span class="o">.</span><span class="n">plot_losses</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/61a8c5b7c582cb4ccf7e8d40269fdf7e9af8f67d8ad1887d20d9a9748b059f64.png" src="../_images/61a8c5b7c582cb4ccf7e8d40269fdf7e9af8f67d8ad1887d20d9a9748b059f64.png" />
</div>
</div>
<p>La conclusión que sacamos después de ver el gráfico anterior es que es horrible, porque parece que nuestro modelo apenas aprende nada.</p>
<p>Tal vez un modelo más profundo pueda hacerlo mejor.</p>
<section id="modelo-mas-profundo">
<h3><span class="section-number">8.11.1. </span>Modelo más profundo<a class="headerlink" href="#modelo-mas-profundo" title="Permalink to this heading">#</a></h3>
<p>Para construir nuestro modelo más profundo, vamos a añadir no una, sino dos capas ocultas a nuestro modelo y hacerlo más profundo. Todavía empezamos con una capa de aplanamiento, y la última parte de nuestro modelo sigue siendo una Sigmoide, pero hay dos capas Lineales antes de la capa de salida ya existente.</p>
<p>La forma esquemática se este modelo seria la siguiente</p>
<p><img alt="Modelo tres capas" src="../_images/ModeloTresCapas.PNG" /></p>
<p>Respecto de la nomenclatura de la figura anterior, los subíndices de w y z representan los índices (de base cero) para la capa y la unidad: en la capa de salida. Asi por ejemplo, w20 representa los pesos correspondientes a la
primera unidad (#0) de la tercera capa (#2).</p>
<p>En este gráfico qué estamos representando? Vamos a resolver el paso hacia adelante(forward pass) , es decir
el camino de las entradas (x) a la salida (y):</p>
<p>1.- Una imagen se aplana a un tensor con 25 características o features, de x0 a x24 (no representado en la figura anterior)</p>
<p>2.- Las 25 características se envían a cada una de las cinco unidades de capa oculta #0</p>
<p>3.- Cada unidad de la capa oculta #0 utiliza sus pesos, de w00 a w04, y las características de la capa de entrada para calcular sus salidas correspondientes, de z00 a z04.</p>
<p>4.- Las salidas de la capa oculta #0 se envían a cada una de las tres unidades de la capa oculta nº 1 (en cierto modo, las salidas de Capa Oculta #0 funcionan como si fueran características de la Capa Oculta Layer #1).</p>
<p>5.- cada unidad de la capa oculta nº 1 utiliza sus pesos, de w10 a w12 y los valores z0 de la capa oculta anterior para calcular sus salidas correspondientes, de z10 a z12.</p>
<p>6.- Las salidas de la capa oculta nº 1 se envían a  la capa de salida (de nuevo, las salidas de la capa oculta nº 1 funcionan como si fueran características de la capa de salida).</p>
<p>7.- La unidad de la capa de salida utiliza sus pesos (w20) y los valores z1 de la capa oculta anterior para calcular su
salida correspondiente (z2).</p>
<p>8.- z2 es un logit, que se convierte en una probabilidad utilizando una función sigmoidea.</p>
<p>Hay un par de cosas a destacar:</p>
<ul class="simple">
<li><p>Todas las unidades de las capas ocultas, y la de la capa de salida, toman un conjunto de entradas (x o z) y realizan la misma operación (wTx o wTz, cada una utilizando sus propios pesos, por supuesto), produciendo una salida (z).</p></li>
<li><p>En las capas ocultas, estas operaciones son exactamente como los modelos de regresión logística que hemos utilizado hasta ahora, modelos en que la regresión logística produjo un logit.</p></li>
<li><p>Es perfectamente correcto pensar en las salidas de una capa como características de la siguiente capa; de hecho, esto es el núcleo de la técnica de que veremos en un capítulo posterior.</p></li>
</ul>
<p>DE acuerdo con todo esto, el modelo que vamos a emplear se construye de la siguiente manera,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sets learning rate - this is &quot;eta&quot; ~ the &quot;n&quot; like Greek letter</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="c1"># Now we can create a model</span>
<span class="n">model_nn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model_nn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;flatten&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model_nn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;hidden0&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_nn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;hidden1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_nn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_nn</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>

<span class="c1"># Defines a SGD optimizer to update the parameters </span>
<span class="n">optimizer_nn</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_nn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># Defines a binary cross entropy loss function</span>
<span class="n">binary_loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora entrenamos el modelo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">sbs_nn</span> <span class="o">=</span> <span class="n">StepByStep</span><span class="p">(</span><span class="n">model_nn</span><span class="p">,</span> <span class="n">binary_loss_fn</span><span class="p">,</span> <span class="n">optimizer_nn</span><span class="p">)</span>
<span class="n">sbs_nn</span><span class="o">.</span><span class="n">set_loaders</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
<span class="n">sbs_nn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sbs_nn</span><span class="o">.</span><span class="n">plot_losses</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/37bccc371243d2dcc44ac2a014a88c3b7f53d767f9f406069fba6efaf22c18bd.png" src="../_images/37bccc371243d2dcc44ac2a014a88c3b7f53d767f9f406069fba6efaf22c18bd.png" />
</div>
</div>
<p>Como podemos observar con este modelo tampoco podemos mejorar sustancialmente la función de pérdida. No hemos mejorado nada. Lo que está ocurriendo es que no hemos metido en el modelo ninguna función de activación.</p>
<p id="index-5">Una <strong>función de activación</strong> es una función no lineal que transforma las salidas de las capas ocultas, de forma similar a la función sigmoidea que transforma los logits en la capa de salida.</p>
<p>En realidad, la sigmoide es una de las muchas funciones de activación. Hay otras, como la hiperbólica-tangente (tanh) y la Rectified Linear Unit (ReLU).</p>
<p>Un modelo más profundo sin funciones de activación en sus capas ocultas no es mejor que una regresión lineal o logística. Eso es lo que quería ilustrar con los dos modelos que hemos entrenado, el superficial y el profundo. Por eso he eliminado el sesgo en ambos modelos también: hace que la comparación sea más directa.</p>
</section>
<section id="el-modelo-matematico-subyacente">
<h3><span class="section-number">8.11.2. </span>El modelo matemático subyacente.<a class="headerlink" href="#el-modelo-matematico-subyacente" title="Permalink to this heading">#</a></h3>
<p>Desarrollamos este apartado por si tienes curiosidad por entender, usando multiplicación de matrices, por qué nuestro modelo profundo es equivalente a una regresión logística, para ello comprueba la secuencia de ecuaciones que aparece a continuación.</p>
<p>En las expresiones matemáticas que siguen, cada fila corresponde a una capa. Los datos fluyen de derecha a izquierda (ya que así es como se multiplicar una secuencia de matrices), empezando por las 25 características a la
derecha y terminando con un único resultado logit a la izquierda.</p>
<p>Si se observa cada capa (fila) por separado, también debería quedar claro que las salidas de una capa determinada (el vector más a la izquierda de cada fila) son las de la siguiente capa, del mismo modo que las características son las entradas de la primera capa.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\large
\begin{array}{rcccccccccccc}
\text{Hidden}\ \#0 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;
\underset{(5 \times 1)}{
\begin{bmatrix}
z_{00} \\
z_{01} \\
z_{02} \\
z_{03} \\
z_{04} \\
\end{bmatrix}}
&amp;
=
&amp;
\underset{(5 \times 25)}{
\begin{bmatrix}
- &amp; w^{T}_{00} &amp; -\\
- &amp; w^{T}_{01} &amp; -\\
- &amp; w^{T}_{02} &amp; -\\
- &amp; w^{T}_{03} &amp; -\\
- &amp; w^{T}_{04} &amp; -
\end{bmatrix}}
&amp;
&amp;
\underset{(25 \times 1)}{
\begin{bmatrix}
x_0 \\
\vdots \\
x_{11} \\
\vdots \\
x_{24}
\end{bmatrix}}
\\
\text{Hidden}\ \#1 &amp; &amp; &amp; &amp;
\underset{(3 \times 1)}{
\begin{bmatrix}
z_{10} \\
z_{11} \\
z_{12} \\
\end{bmatrix}}
&amp;
=
&amp;
\underset{(3 \times 5)}{
\begin{bmatrix}
- &amp; w^{T}_{10} &amp; -\\
- &amp; w^{T}_{11} &amp; -\\
- &amp; w^{T}_{12} &amp; -\\
\end{bmatrix}}
&amp;
&amp;
\underset{(5 \times 1)}{
\begin{bmatrix}
z_{00} \\
z_{01} \\
z_{02} \\
z_{03} \\
z_{04} \\
\end{bmatrix}}
\\
\text{Output} &amp;
\underset{(1 \times 1)}{
\begin{bmatrix}
z_{2}
\end{bmatrix}}
&amp;
=
&amp;
\underset{(1 \times 3)}{
\begin{bmatrix}
- &amp; w^{T}_{20} &amp; -\\
\end{bmatrix}}
&amp;
\underset{(3 \times 1)}{
\begin{bmatrix}
z_{10} \\
z_{11} \\
z_{12} \\
\end{bmatrix}}
\\
\hline
\text{substituyendo } z's... &amp;
\underset{(1 \times 1)}{
\begin{bmatrix}
z_{2}
\end{bmatrix}}
&amp;
=
&amp;
\underbrace{
\underset{(1 \times 3)}{
\begin{bmatrix}
- &amp; w^{T}_{20} &amp; -\\
\end{bmatrix}}}_{\text{Output Layer}}
&amp;
&amp; &amp;
\underbrace{
\underset{(3 \times 5)}{
\begin{bmatrix}
- &amp; w^{T}_{10} &amp; -\\
- &amp; w^{T}_{11} &amp; -\\
- &amp; w^{T}_{12} &amp; -\\
\end{bmatrix}}}_{\text{Hidden Layer #1}}
&amp; &amp;
&amp; &amp;
\underbrace{
\underset{(5 \times 25)}{
\begin{bmatrix}
- &amp; w^{T}_{00} &amp; -\\
- &amp; w^{T}_{01} &amp; -\\
- &amp; w^{T}_{02} &amp; -\\
- &amp; w^{T}_{03} &amp; -\\
- &amp; w^{T}_{04} &amp; -
\end{bmatrix}}}_{\text{Hidden Layer #0}}
&amp;
&amp;
\underset{(25 \times 1)}{
\begin{bmatrix}
x_0 \\
\vdots \\
x_{11} \\
\vdots \\
x_{24}
\end{bmatrix}}
\\
\text{multiplicando...} &amp; &amp;
=
&amp;
\underbrace{
\underset{(1 \times 25)}{
\begin{bmatrix}
- &amp; w^{T} &amp; -\\
\end{bmatrix}}}_{\text{Matrices Multiplicadas}}
&amp;
&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;
\underset{(25 \times 1)}{
\begin{bmatrix}
x_0 \\
\vdots \\
x_{11} \\
\vdots \\
x_{24}
\end{bmatrix}}
\end{array}
\end{split}\]</div>
<p>De acuerdo con estos resultados, <strong>se puede concluir que un modelo con cualquier número de capas ocultas tiene un
modelo equivalente al de sin capas ocultas</strong>. Claro, no incluimos el sesgo aquí porque haría mucho más difícil ilustrar este
punto. Por este motivo los gráficos que hemos obtenido en nuestro ejemplo mostrado más arriba, los resultados de la función de pérdida eran muy similares.</p>
<p>Si las ecuaciones matriciales mostradas más arriba no fueran suficientes para ver esto, vamos a intentarlo usando algo de código.</p>
<p id="index-6">En primer lugar, tenemos que obtener los pesos para las capas en nuestro modelo profundo. Podemos usar el atributo de peso de cada capa, <em>sin olvidar separarlo() del gráfico de cálculo</em>, para que así lo podamos usarl libremente en otras operaciones.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_nn_hidden0</span> <span class="o">=</span> <span class="n">model_nn</span><span class="o">.</span><span class="n">hidden0</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">w_nn_hidden1</span> <span class="o">=</span> <span class="n">model_nn</span><span class="o">.</span><span class="n">hidden1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">w_nn_output</span> <span class="o">=</span> <span class="n">model_nn</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

<span class="n">w_nn_hidden0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w_nn_hidden1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w_nn_output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([5, 25]), torch.Size([3, 5]), torch.Size([1, 3]))
</pre></div>
</div>
</div>
</div>
<p>Las dimensiones deben coincidir tanto con la definición de nuestro modelo como con las matrices de peso en las ecuaciones definidas anteriormente.</p>
<p>Podemos calcular la fila inferior, es decir, el modelo equivalente utilizando la multiplicación de matrices (que se realiza de derecha a izquierda, como en las ecuaciones):</p>
<p>NOTA: La multiplicación de matrices se hace mediante el símbolo &#64;.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_nn_equiv</span> <span class="o">=</span> <span class="n">w_nn_output</span> <span class="o">@</span> <span class="n">w_nn_hidden1</span> <span class="o">@</span> <span class="n">w_nn_hidden0</span>
<span class="n">w_nn_equiv</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 25])
</pre></div>
</div>
</div>
</div>
<p>A continuación, tenemos que compararlos con las ponderaciones del modelo superficial, es decir, la regresión logística:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_logistic_output</span> <span class="o">=</span> <span class="n">model_logistic</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="n">w_logistic_output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 25])
</pre></div>
</div>
</div>
</div>
<p>La misma dimensión, como se esperaba. Si comparamos los valores, uno por uno, encontraremos que son similares, pero no exactamente iguales. Intentemos de comprender el cuadro completo observando la imagen siguiente:</p>
<p><img alt="Comparación de modelos" src="../_images/ComparacionModelo.PNG" /></p>
<p>A la izquierda, representamos los 25 pesos/parámetros de ambos modelos. Aunque no son exactamente iguales, la similitud es sorprendente.</p>
<p>A la derecha, podemos apreciar que las ponderaciones están, efectivamente, muy correlacionadas.</p>
<p>Es bastante sencillo saber que el modelo de regresión logística de una sola capa tenga 25 ponderaciones. Pero, ¿cuántas ponderaciones tiene el modelo de profundidad, el segundo modelo? Podemos calcularlo: 25 características por cinco unidades en la capa oculta #0:(125), más esas cinco unidades por tres unidades en la capa oculta nº 1 (15),más los tres últimos pesos de la capa oculta nº 1 a la capa de salida de salida, sumando un total de 143 ponderaciones.</p>
<p>O simplemente podríamos usar la propiedad <em>numel</em> de PyTorch para devolver el número total de elementos de elementos, que en este caso será  un tensor.Para conseguir estos creamos la siguiente función y se lo añadimos a la clase <em>StepByStep</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">count_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="c1"># Lo añadimos como método a la clase StepByStep</span>
<span class="nb">setattr</span><span class="p">(</span><span class="n">StepByStep</span><span class="p">,</span> <span class="s1">&#39;count_parameters&#39;</span><span class="p">,</span> <span class="n">count_parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Como hemos dicho, se ha agregado como un método de nuestra clase <em>StepByStep</em>, y tomemos sólo los tensores que requieren gradiente, de modo que sólo contemos con los pesos que necesitan ser actualizados. Ahora mismo, son todos, claro, pero no necesariamente siempre será así y entonces no hará falta hacer esta distinción porque se utilizará  el aprendizaje por transferencia que se verá en un capítulo posterior.</p>
<p>Hacemos este cómputo con el nuevo método</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sbs_logistic</span><span class="o">.</span><span class="n">count_parameters</span><span class="p">(),</span> <span class="n">sbs_nn</span><span class="o">.</span><span class="n">count_parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(25, 143)
</pre></div>
</div>
</div>
</div>
</section>
<section id="pesos-como-pixeles">
<h3><span class="section-number">8.11.3. </span>Pesos como pixeles.<a class="headerlink" href="#pesos-como-pixeles" title="Permalink to this heading">#</a></h3>
<p>Durante la preparación de los datos, aplanamos las entradas de imágenes de cinco por cinco a tensores de 25 elementos. He aquí una idea descabellada: ¿qué pasa si si tomamos algún otro tensor con 25 elementos e intentamos visualizarlo
como una imagen?.</p>
<p>Tenemos algunos candidatos perfectos para esto: los pesos utilizados por cada unidad en la capa oculta #0. Cada unidad utiliza 25 pesos ya que cada unidad recibe valores de 25 características. Incluso tenemos estos pesos en una variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_nn_hidden0</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([5, 25])
</pre></div>
</div>
</div>
</div>
<p>Cinco unidades, 25 pesos cada una. Perfecto. Sólo tenemos que usar la vista para convertir los tensores largos de 25 elementos que representan los pesos en tensores bidimensionales (5x5), y visualizarlos como si fueran imágenes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">figure7</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$w_{0&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;}$&#39;</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Hidden Layer #0&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span> 
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">figure7</span><span class="p">(</span><span class="n">w_nn_hidden0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/824b68bcd2d0b0ddb0be7688a7b3563d383a2073a52515717ef02de2a472fe9c.png" src="../_images/824b68bcd2d0b0ddb0be7688a7b3563d383a2073a52515717ef02de2a472fe9c.png" />
</div>
</div>
<p>La visualización de los pesos en forma de imágenes es habitual cuando se utilizan redes neuronales convolucionales (CNN). Estas imágenes serán llamadas filtros, y los modelos entrenados probablemente exhibirán características reconocibles en sus filtros.</p>
<p>Como nuestro modelo estaba mal entrenado, no es de extrañar que las imágenes de arriba no sean muy informativas. Además, en nuestro caso, no son del todo “filtros”, ya que tienen el mismo tamaño que la imagen de entrada. En los modelos basados en CNN los filtros reales sólo cubren una parte de la imagen. Volveremos a ello en un capítulo posterior.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="funciones-de-activacion">
<h1><span class="section-number">9. </span>Funciones de activación.<a class="headerlink" href="#funciones-de-activacion" title="Permalink to this heading">#</a></h1>
<p id="index-7">Las <strong>funciones de activación</strong> son funciones no lineales. O bien aplastan o doblan las líneas rectas. Rompen la equivalencia entre el modelo profundo y el superficial.</p>
<p>Pytorch cuenta con un buen conjunto de <a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity" target="_blank"> funciones de activación </a>, pero aquí nos vamos a central en las que se utilizan de una forma más frecuente.</p>
<section id="funcion-sigmoidea">
<h2><span class="section-number">9.1. </span>Función sigmoidea<a class="headerlink" href="#funcion-sigmoidea" title="Permalink to this heading">#</a></h2>
<p>Comencemos con la más tradicional de las funciones de activación, la sigmoidea, que ya hemos utilizado para transformar logits en probabilidades. Hoy en día, ese es prácticamente su único uso, pero en los primeros tiempos de las redes neuronales, se encontraba en todas partes.</p>
<div class="math notranslate nohighlight">
\[
\Large \sigma(z) = \frac{1}{1 + e^{-z}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_activation</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">z</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">func</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">sig</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="c1"># Move left y-axis and bottim x-axis to centre, passing through (0,0)</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;tanh&#39;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;relu&#39;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">5.01</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">5.01</span><span class="p">])</span>
        
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma(z)$&#39;</span><span class="p">)</span>

    <span class="c1"># Eliminate upper and right axes</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

    <span class="c1"># Show ticks in the left and lower axes only</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">sig</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Activation&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">z</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gradient&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plot_activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Francisco\AppData\Local\Temp\ipykernel_8872\1908806294.py:43: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  fig.show()
</pre></div>
</div>
<img alt="../_images/ad3d42f908df5391b525cb23f5645f2e14ea8b8a4abdf4f0b4fc35435d384997.png" src="../_images/ad3d42f908df5391b525cb23f5645f2e14ea8b8a4abdf4f0b4fc35435d384997.png" />
</div>
</div>
<p>Recapitulemos rápidamente la forma de una sigmoide: como puede ver en la figura anterior, una función de activación sigmoidea “aplasta” sus valores de entrada de entrada (z) en el rango (0, 1) (el mismo rango pueden tomar las probabilidades, la razón por la que se utiliza en la capa de salida para tareas de clasificación binaria
binaria). También se puede comprobar que <strong>el valor de su pico de gradiente es sólo 0,25</strong> (para z = 0) y que se acerca ya a cero a medida que el valor absoluto de z alcanza un valor de cinco.</p>
<p>Además, recuerde que los valores de activación de cualquier capa son las entradas de la capa siguiente y, dado el rango del sigmoide los valores de activación van a estar centrados alrededor de 0,5, en lugar de de cero. Esto significa que, aunque normalicemos nuestras entradas para alimentar la primera capa, ya no será así para las demás capas.</p>
<p>En los capítulos anteriores, estandarizamos las características (media cero, desviación estándar) para mejorar el rendimiento del descenso gradiente. El mismo razonamiento se aplica aquí, ya que las salidas de cualquier de cualquier capa son las entradas de la capa siguiente. En realidad hay más, y tocaremos brevemente este tema de nuevo en la función de activación ReLU.</p>
<p>PyTorch tiene la función sigmoide disponible de dos formas posibles: <em>torch.sigmoide y nn.Sigmoide</em>. La primera
es una función simple, y la segunda es una clase completa heredada de nn.Module, siendo así, a todos los efectos, un
modelo en sí mismo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">dummy_z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0474, 0.5000, 0.9526])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()(</span><span class="n">dummy_z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0474, 0.5000, 0.9526])
</pre></div>
</div>
</div>
</div>
</section>
<section id="tangente-hiperbolica-tanh">
<h2><span class="section-number">9.2. </span>Tangente hiperbólica (TanH).<a class="headerlink" href="#tangente-hiperbolica-tanh" title="Permalink to this heading">#</a></h2>
<p id="index-8">La función de activación <strong>tangente hiperbólica</strong> es la evolución de la sigmoidea, ya que sus salidas son valores con media cero, a diferencia de su predecesora.</p>
<div class="math notranslate nohighlight">
\[
\Large \sigma(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plot_activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Francisco\AppData\Local\Temp\ipykernel_8872\1908806294.py:43: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  fig.show()
</pre></div>
</div>
<img alt="../_images/d6cdcb055af5327f48293e0ee55068ad82eacff1c41caabd1e908fcb63041a02.png" src="../_images/d6cdcb055af5327f48293e0ee55068ad82eacff1c41caabd1e908fcb63041a02.png" />
</div>
</div>
<p>Como puede ver en la figura anterior, la función de activación tanh “aplasta” los valores de entrada en el rango (-1, 1). Por lo tanto, al estar centrados en cero, los valores de activación ya están (en cierto modo)  normalizadas para la siguiente capa, lo que hace que la tangente hiperbólica una mejor función de activación que la sigmoidea.</p>
<p>En cuanto al gradiente, tiene un valor máximo mucho mayor de 1,0 (de nuevo, para z = 0), pero su disminución es aún más rápida, acercándose a cero a valores absolutos de z tan bajos como tres. Esta es la causa subyacente de lo que se denomina el <em>problema de los gradientes de fuga</em>, que hace que el entrenamiento de la red sea progresivamente más lento.</p>
<p>En PyTorch se puede obtener de la siguiente manera</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">dummy_z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.9951,  0.0000,  0.9951])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()(</span><span class="n">dummy_z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.9951,  0.0000,  0.9951])
</pre></div>
</div>
</div>
</div>
</section>
<section id="rectified-linear-unit-relu">
<h2><span class="section-number">9.3. </span>Rectified Linear Unit (ReLU)<a class="headerlink" href="#rectified-linear-unit-relu" title="Permalink to this heading">#</a></h2>
<p id="index-9">Tal vez “aplastar” no sea el camino a seguir… ¿y si doblamos un poco las reglas y utilizamos una función de activación que  doble la línea? La ReLU nació así y dio lugar a toda una familia de funciones similares.</p>
<p>La ReLU, o una de sus parientes, es la función de activación más común hoy en día. Se trata de una función de activación que resuelve el problema de la desaparición de los gradientes de las variables y también es la más rápida para calcular los gradientes.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Large
\begin{aligned}
\sigma(z) &amp;=
\begin{cases}
z,\ \text{if } z \ge 0
\\
0,\ \text{if } z &lt; 0
\end{cases}
\\
&amp; \text{or}
\\
\sigma(z) &amp;= \text{max}(0, z)
\end{aligned}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plot_activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Francisco\AppData\Local\Temp\ipykernel_8872\1908806294.py:43: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  fig.show()
</pre></div>
</div>
<img alt="../_images/6ee366b6fe1adfc6a0d0ab049380376761c8e3501a2ab3778b0273e05ebaf145.png" src="../_images/6ee366b6fe1adfc6a0d0ab049380376761c8e3501a2ab3778b0273e05ebaf145.png" />
</div>
</div>
<p>Como puede ver en la figura anterior, la función ReLU es totalmente diferente a las que hemos visto anteriormente: no “aplasta” los valores en un rango, simplemente preserva los valores positivos y convierte todos los valores negativos en cero.</p>
<p>La ventaja de utilizar una ReLU es que su gradiente es uno (para valores positivos) o cero (para valores negativos) - ¡no más gradientes gradientes que se desvanecen. Este patrón conduce a una convergencia más rápida de la red.</p>
<p>Por otro lado, este comportamiento también puede conducir a lo que se denomina una <strong>“neurona muerta”</strong>, es decir, una neurona cuyas entradas son son sistemáticamente negativas y, por tanto, siempre tiene un valor de activación  de cero. Peor aún, el gradiente también es cero para las entradas negativas, lo que significa que los pesos no se actualizan. Es como si la neurona se hubiera atascado.</p>
<p>Los valores de activación de la ReLU no están centrados obviamente en cero. ¿Es peor que la tangente hiperbólica? Seguramente no, de lo contrario no se habría convertido en una función de activación tan popular entre los profesionales.</p>
<p>La ReLU, al tener sus gradientes comparativamente mayores, es capaz de lograr resultados mejores y más rápidos que las otras dos funciones de activación a pesar de que sus salidas no estén centradas en cero.</p>
<p>En PyTorch hay tres caminos para obtener esta función de activación</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">dummy_z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0., 3.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()(</span><span class="n">dummy_z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0., 3.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_z</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0., 3.])
</pre></div>
</div>
</div>
</div>
</section>
<section id="leaky-relu">
<h2><span class="section-number">9.4. </span>Leaky ReLU<a class="headerlink" href="#leaky-relu" title="Permalink to this heading">#</a></h2>
<p id="index-10">¿Cómo se puede dar a una “neurona muerta” la oportunidad de volver a la vida? Si el problema subyacente es el hecho de que se ha atascado, tenemos que dar un pequeño empujón. Y eso es lo que hace una ReLU con fugas (ReLU Leaky): para entradas negativas, devuelve un pequeño valor de activación y gradiente, en lugar de un cero fijo para ambos. El multiplicador para
valores negativos, 0,01, se llama <em>coeficiente de fuga</em>.</p>
<p>Puede que no sea mucho, pero da a la neurona la oportunidad de despegarse, y mantiene las buenas propiedades de la ReLU: mayores gradientes y una convergencia más rápida.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Large
\begin{aligned}
\sigma(z) =&amp;
\begin{cases}
z,\ \text{if } z \ge 0
\\
0.01z,\ \text{if } z &lt; 0
\end{cases}
\\
\text{or}&amp;
\\
\sigma(z)=&amp;\text{max}(0,z)+0.01\ \text{min}(0,z)
\end{aligned}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plot_activation</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Leaky ReLU&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Francisco\AppData\Local\Temp\ipykernel_8872\1908806294.py:43: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  fig.show()
</pre></div>
</div>
<img alt="../_images/9847e0920f074e1ba89f0206b60982e98a7e42a01847f5e52bf530579f658530.png" src="../_images/9847e0920f074e1ba89f0206b60982e98a7e42a01847f5e52bf530579f658530.png" />
</div>
</div>
<p>Su valor se puede conseguir de las siguientes formas</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">F</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">dummy_z</span><span class="p">,</span> <span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.0300,  0.0000,  3.0000])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)(</span><span class="n">dummy_z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.0600,  0.0000,  3.0000])
</pre></div>
</div>
</div>
</div>
</section>
<section id="parametric-relu-prelu">
<h2><span class="section-number">9.5. </span>Parametric ReLU (PReLU)<a class="headerlink" href="#parametric-relu-prelu" title="Permalink to this heading">#</a></h2>
<p id="index-11">La función de activación denominada <em>ReLU paramétrica</em> es la evolución natural del ReLU con fugas: en lugar de elegir arbitrariamente un coeficiente de fuga (como 0,01), hagamos que sea un parámetro (a). Con suerte, el modelo aprenderá
cómo evitar las neuronas muertas, o cómo devolverlas a la vida. Esta función se podría decir que, es una solución ingeniosa al problema.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Large
\begin{aligned}
\sigma(z) =&amp;
\begin{cases}
z,\ \text{if } z \ge 0
\\
az,\ \text{if } z &lt; 0
\end{cases}
\\
\text{or}&amp;
\\
\sigma(z)=&amp;\text{max}(0,z)+a\ \text{min}(0,z)
\end{aligned}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plot_activation</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Parametric ReLU&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Francisco\AppData\Local\Temp\ipykernel_8872\1908806294.py:43: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  fig.show()
</pre></div>
</div>
<img alt="../_images/f17ad0608e3d5c8c31d6f16bf21eb8ed8f75f980ee74ddafe1d426864d7d7571.png" src="../_images/f17ad0608e3d5c8c31d6f16bf21eb8ed8f75f980ee74ddafe1d426864d7d7571.png" />
</div>
</div>
<p>Como se puede ver en la figura anterior, la pendiente del lado izquierdo es mucho mayor ahora, 0,25 para ser precisos, el valor por defecto de PyTorch para el parámetro a. Su valor se puede conseguir de la siguiente forma en PyTorch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">F</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="n">dummy_z</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.25</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.7500,  0.0000,  3.0000])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)(</span><span class="n">dummy_z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.7500,  0.0000,  3.0000], grad_fn=&lt;PreluKernelBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="modelo-de-aprendizaje">
<h1><span class="section-number">10. </span>Modelo de aprendizaje.<a class="headerlink" href="#modelo-de-aprendizaje" title="Permalink to this heading">#</a></h1>
<p>Ahora que hemos aprendido que las funciones de activación rompen la equivalencia con un modelo superficial, vamos a utilizarlas para transformar nuestro modelo profundo en otro modelo profundo, pero que resuelve mejor el problema. Tiene la misma arquitectura que el modelo anterior, excepto por la incorporación de las funciones de activación aplicadas a las salidas de las capas ocultas. Este es el diagrama del modelo actualizado:</p>
<p><img alt="Modelo con Función Activación" src="../_images/ModeloConFuncionActivacion.PNG" /></p>
<p>A continuación mostramos el código necesario para implementar ese modelo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sets learning rate - this is &quot;eta&quot; ~ the &quot;n&quot; like Greek letter</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
<span class="c1"># Now we can create a model</span>
<span class="n">model_relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;flatten&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;hidden0&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;activation0&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;hidden1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;activation1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>

<span class="c1"># Defines a SGD optimizer to update the parameters</span>
<span class="n">optimizer_relu</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_relu</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># Defines a binary cross entropy loss function</span>
<span class="n">binary_loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Entrenamos el modelo</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">sbs_relu</span> <span class="o">=</span> <span class="n">StepByStep</span><span class="p">(</span><span class="n">model_relu</span><span class="p">,</span> <span class="n">binary_loss_fn</span><span class="p">,</span> <span class="n">optimizer_relu</span><span class="p">)</span>
<span class="n">sbs_relu</span><span class="o">.</span><span class="n">set_loaders</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
<span class="n">sbs_relu</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">sbs_relu</span><span class="o">.</span><span class="n">plot_losses</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/75e882cb16e3707465c4061dd2c3db81440161ed43f127ff603d3b1815a2e8c4.png" src="../_images/75e882cb16e3707465c4061dd2c3db81440161ed43f127ff603d3b1815a2e8c4.png" />
</div>
</div>
<p>Esto está mucho mejor. Pero, para comprender realmente la diferencia que suponen las funciones de activación, vamos a trazar todos los modelos en el mismo gráfico:</p>
<p><img alt="Todos gráficos" src="../_images/DosGraficos.PNG" /></p>
<p>Nuestro último nuevo modelo sólo necesitó una pequeña cantidad de iteraciones  para superar al anterior.Claramente, este modelo no es equivalente a una regresión logística: es mucho mejor.</p>
<p>No obstante aunque se mejora el resultado, ambos modelos son un poco cutres pues su rendimiento es bastante pobre si se observan sus precisiones (que oscilan entre el 43% y el 65% para el conjunto de validación). El único propósito de este
ejercicio es demostrar que las funciones de activación, rompiendo la equivalencia con una regresión logística, son capaces de lograr mejores resultados en la minimización de la función de  pérdidas.</p>
<p>Este último modelo en particular también presenta una pérdida de validación menor que la pérdida de entrenamiento, que no es lo que generalmente se espera. Ya hemos  un caso como éste en un capítulo anterior: el conjunto de validación era más fácil que que el conjunto de entrenamiento. El ejemplo actual es un poco más matizado que que eso… aquí está la explicación.</p>
<p>En primer lugar, nuestro modelo no es tan bueno y tiene tendencia a predecir más puntos en la clase positiva (FPR y TPR altos);</p>
<p>En segundo lugar, uno de los mini lotes del conjunto de validación tiene casi todos sus puntos en la clase positiva, por lo que la pérdida es muy baja; en tercer lugar sólo hay cuatro minilotes en el conjunto de validación, por lo que la
pérdida media se ve fácilmente afectada por un solo minilote.</p>
<p>Es hora de hacerse dos preguntas:</p>
<ul class="simple">
<li><p>¿por qué se rompe la equivalencia con una regresión logística?</p></li>
<li><p>¿qué hacen exactamente las funciones de activación bajo el capó?</p></li>
</ul>
<p>La primera pregunta la respondemos en el siguiente apartado utilizando para ello las fórmulas matemáticas que hay debajo de este modelo. La segunda pregunta se responderá en un capítulo posterior.</p>
<section id="formulacion-matematica-del-problema">
<h2><span class="section-number">10.1. </span>Formulación matemática del problema.<a class="headerlink" href="#formulacion-matematica-del-problema" title="Permalink to this heading">#</a></h2>
<div class="math notranslate nohighlight">
\[\begin{split}
\large
\begin{array}{rcccccccccccc}
\text{Hidden }\#0 &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;
\underset{(5 \times 1)}{
\begin{bmatrix}
z_{00} \\
z_{01} \\
z_{02} \\
z_{03} \\
z_{04} \\
\end{bmatrix}}
&amp;
=
&amp;
\underset{(5 \times 25)}{
\begin{bmatrix}
- &amp; w^{T}_{00} &amp; -\\
- &amp; w^{T}_{01} &amp; -\\
- &amp; w^{T}_{02} &amp; -\\
- &amp; w^{T}_{03} &amp; -\\
- &amp; w^{T}_{04} &amp; -
\end{bmatrix}}
&amp;
&amp;
\underset{(25 \times 1)}{
\begin{bmatrix}
x_0 \\
\vdots \\
x_{11} \\
\vdots \\
x_{24}
\end{bmatrix}}
\\
\text{Hidden }\#1 &amp; &amp; &amp; &amp;
\underset{(3 \times 1)}{
\begin{bmatrix}
z_{10} \\
z_{11} \\
z_{12} \\
\end{bmatrix}}
&amp;
=
&amp;
\underset{(3 \times 5)}{
\begin{bmatrix}
- &amp; w^{T}_{10} &amp; -\\
- &amp; w^{T}_{11} &amp; -\\
- &amp; w^{T}_{12} &amp; -\\
\end{bmatrix}}
&amp;
&amp;
\underbrace{
f_0
\underset{(5 \times 1)}{
\left(
\begin{bmatrix}
z_{00} \\
z_{01} \\
z_{02} \\
z_{03} \\
z_{04} \\
\end{bmatrix}
\right)}}_{\text{Activation #0}}
\\
\text{Output} &amp;
\underset{(1 \times 1)}{
\begin{bmatrix}
z_{2}
\end{bmatrix}}
&amp;
=
&amp;
\underset{(1 \times 3)}{
\begin{bmatrix}
- &amp; w^{T}_{20} &amp; -\\
\end{bmatrix}}
&amp;
\underbrace{
f_1
\underset{(3 \times 1)}{
\left(
\begin{bmatrix}
z_{10} \\
z_{11} \\
z_{12} \\
\end{bmatrix}
\right)}}_{\text{Activation #1}}
\\
\hline
\text{substituting z's...} &amp;
\underset{(1 \times 1)}{
\begin{bmatrix}
z_{2}
\end{bmatrix}}
&amp;
=
&amp;
\underbrace{
\underset{(1 \times 3)}{
\begin{bmatrix}
- &amp; w^{T}_{20} &amp; -\\
\end{bmatrix}}}_{\text{Output Layer}}
&amp;
f_1
&amp; &amp;
\left(
\underbrace{
\underset{(3 \times 5)}{
\begin{bmatrix}
- &amp; w^{T}_{10} &amp; -\\
- &amp; w^{T}_{11} &amp; -\\
- &amp; w^{T}_{12} &amp; -\\
\end{bmatrix}}}_{\text{Hidden Layer #1}}
\right.
&amp;
&amp;
f_0
&amp; &amp;
\left(
\underbrace{
\underset{(5 \times 25)}{
\begin{bmatrix}
- &amp; w^{T}_{00} &amp; -\\
- &amp; w^{T}_{01} &amp; -\\
- &amp; w^{T}_{02} &amp; -\\
- &amp; w^{T}_{03} &amp; -\\
- &amp; w^{T}_{04} &amp; -
\end{bmatrix}}}_{\text{Hidden Layer #0}}
\right.
&amp;
&amp;
\left.
\left.
\underbrace{
\underset{(25 \times 1)}{
\begin{bmatrix}
x_0 \\
\vdots \\
x_{11} \\
\vdots \\
x_{24}
\end{bmatrix}}}_{\text{Inputs}}
\right)
\right)
\end{array}
\end{split}\]</div>
<p>Como antes, los datos fluyen de derecha a izquierda (ya que así es como se multiplicar una secuencia de matrices), empezando por las 25 características a la la derecha y terminando con una única salida logit a la izquierda.</p>
<p>Si se observa cada capa (fila) por separado, también debería quedar claro que las salidas de una capa determinada (el vector más a la izquierda de la fila) son transformadas por una función de activación antes de convertirse en entradas
de la siguiente capa.</p>
<p>La fila que está debajo de la línea muestra el resultado de componer todas las operaciones por encima de la línea. No hay forma de simplificar más la expresión debido a la existencia de las dos funciones de activación (f0 y f1). En efecto, rompen la equivalencia con una regresión logística.</p>
<p>Si ponemos todo el código final obtenido en este capítulo nos quedará lo siguiente</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformedTensorDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>
        
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">index_splitter</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">splits</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="c1"># Makes the split argument a tensor</span>
    <span class="n">splits_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">splits</span><span class="p">)</span>
    <span class="c1"># Finds the correct multiplier, so we don&#39;t have</span>
    <span class="c1"># to worry about summing up to N (or one)</span>
    <span class="n">multiplier</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">splits_tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>    
    <span class="n">splits_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">multiplier</span> <span class="o">*</span> <span class="n">splits_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
    <span class="c1"># If there is a difference, throws at the first split</span>
    <span class="c1"># so random_split does not complain</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">splits_tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">splits_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">diff</span>
    <span class="c1"># Uses PyTorch random_split to split the indices</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">random_split</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">splits_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_balanced_sampler</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="c1"># Computes weights for compensating imbalanced classes</span>
    <span class="n">classes</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">counts</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">sample_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">long</span><span class="p">()]</span>
    <span class="c1"># Builds sampler with compute weights</span>
    <span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">WeightedRandomSampler</span><span class="p">(</span>
        <span class="n">weights</span><span class="o">=</span><span class="n">sample_weights</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_weights</span><span class="p">),</span>
        <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
        <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">sampler</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Builds tensors from numpy arrays BEFORE split</span>
<span class="c1"># Modifies the scale of pixel values from [0, 255] to [0, 1]</span>
<span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">images</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">y_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="c1"># Uses index_splitter to generate indices for training and</span>
<span class="c1"># validation sets</span>
<span class="n">train_idx</span><span class="p">,</span> <span class="n">val_idx</span> <span class="o">=</span> <span class="n">index_splitter</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">),</span> <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
<span class="c1"># Uses indices to perform the split</span>
<span class="n">x_train_tensor</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">y_train_tensor</span> <span class="o">=</span> <span class="n">y_tensor</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">x_val_tensor</span> <span class="o">=</span> <span class="n">x_tensor</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span>
<span class="n">y_val_tensor</span> <span class="o">=</span> <span class="n">y_tensor</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span>

<span class="c1"># Builds different composers because of data augmentation on training set</span>
<span class="n">train_composer</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">.5</span><span class="p">),</span>
                          <span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,))])</span>
<span class="n">val_composer</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">.5</span><span class="p">,))])</span>
<span class="c1"># Uses custom dataset to apply composed transforms to each set</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TransformedTensorDataset</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">,</span> <span class="n">y_train_tensor</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">train_composer</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">TransformedTensorDataset</span><span class="p">(</span><span class="n">x_val_tensor</span><span class="p">,</span> <span class="n">y_val_tensor</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">val_composer</span><span class="p">)</span>

<span class="c1"># Builds a weighted random sampler to handle imbalanced classes</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">make_balanced_sampler</span><span class="p">(</span><span class="n">y_train_tensor</span><span class="p">)</span>

<span class="c1"># Uses sampler in the training set to get a balanced data loader</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sets learning rate - this is &quot;eta&quot; ~ the &quot;n&quot; like Greek letter</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>
<span class="c1"># Now we can create a model</span>
<span class="n">model_relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;flatten&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;hidden0&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;activation0&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;hidden1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;activation1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model_relu</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>

<span class="c1"># Defines a SGD optimizer to update the parameters</span>
<span class="c1"># (now retrieved directly from the model)</span>
<span class="n">optimizer_relu</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_relu</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># Defines a binary cross entropy loss function</span>
<span class="n">binary_loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">sbs_relu</span> <span class="o">=</span> <span class="n">StepByStep</span><span class="p">(</span><span class="n">model_relu</span><span class="p">,</span> <span class="n">binary_loss_fn</span><span class="p">,</span> <span class="n">optimizer_relu</span><span class="p">)</span>
<span class="n">sbs_relu</span><span class="o">.</span><span class="n">set_loaders</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
<span class="n">sbs_relu</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./jupyters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="Tema_3_clasificacion.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Introducción problemas de clasificación.</p>
      </div>
    </a>
    <a class="right-next"
       href="ModelosPreentrenados.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Introducción. Modelos preentrenados.</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">8. Introducción clasificación de imágenes.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formacion-de-imagenes-para-tensores">8.1. Formación de imágenes para tensores.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formato-nchw-vs-nhwc">8.2. Formato NCHW vs NHWC.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torchvision">8.3. Torchvision.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets">8.3.1. Datasets.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos">8.3.2. Modelos.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformadores">8.3.3. Transformadores.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformaciones-sobre-imagenes">8.3.4. Transformaciones sobre imágenes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformaciones-sobre-tensores">8.3.5. Transformaciones sobre tensores.</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformacion-normalize">8.3.5.1. Transformación normalize.</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#composicion-de-transformaciones">8.3.6. Composición de transformaciones.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparacion-de-los-datos">8.4. Preparación de los datos.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subsetrandomsampler">8.5. SubsetRandomSampler.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformacion-para-aumentar-los-datos">8.6. Transformación para aumentar los datos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weightedrandomsampler">8.7. WeightedRandomSampler.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#las-semillas-seeds">8.8. Las semillas (seeds)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pixeles-y-features">8.9. Pixeles y features.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-shallow-modelo-poco-profundo">8.10. Modelo Shallow (modelo poco profundo).</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notacion-utilizada">8.11. Notación utilizada.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-mas-profundo">8.11.1. Modelo más profundo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#el-modelo-matematico-subyacente">8.11.2. El modelo matemático subyacente.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pesos-como-pixeles">8.11.3. Pesos como pixeles.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#funciones-de-activacion">9. Funciones de activación.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#funcion-sigmoidea">9.1. Función sigmoidea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tangente-hiperbolica-tanh">9.2. Tangente hiperbólica (TanH).</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rectified-linear-unit-relu">9.3. Rectified Linear Unit (ReLU)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu">9.4. Leaky ReLU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-relu-prelu">9.5. Parametric ReLU (PReLU)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-de-aprendizaje">10. Modelo de aprendizaje.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formulacion-matematica-del-problema">10.1. Formulación matemática del problema.</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Francisco Rodríguez
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=927b94d3fcb96560df09"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=927b94d3fcb96560df09"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>